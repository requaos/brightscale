version: "3.9"

services:
  spark-master:
    build:
      context: .
      dockerfile: ./docker/Dockerfile.delta-spark_scala2.13
    ports:
      - "9090:8080"
      - "7077:7077"
    volumes:
      - ./conf/master:/conf
      - ./dist:/opt/spark-apps
      - ./dist:/opt/spark/work
    environment:
      - SPARK_LOCAL_IP=spark-master
      - SPARK_WORKLOAD=master
      - SPARK_CONF_DIR=/conf
    
  spark-worker:
    build:
      context: .
      dockerfile: ./docker/Dockerfile.delta-spark_scala2.13
    deploy:
      replicas: 3
    ports:
      - "8080"
      - "7000"
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
      - SPARK_DRIVER_MEMORY=1G
      - SPARK_EXECUTOR_MEMORY=1G
      - SPARK_WORKLOAD=worker
      - SPARK_LOCAL_IP=spark-worker
      - SPARK_CONF_DIR=/conf
    volumes:
      - ./conf/worker:/conf
      - ./dist:/opt/spark-apps
      - ./dist:/opt/spark/work

  spark-console:
    build:
      context: .
      dockerfile: ./docker/Dockerfile.delta-spark_scala2.13
    volumes:
      - ./dist:/opt/spark-apps
      - ./data/delta-streaming:/data/delta-streaming
    depends_on:
      - spark-master
    environment:
      - SPARK_WORKLOAD=submit

  scala-jupyter:
    build:
      context: .
      dockerfile: ./docker/Dockerfile.jupyter
    volumes:
      - ./data/notebook-bucket:/data/notebook-bucket
    ports:
      - "8888:8888"

  s3-like:
    build:
      context: .
      dockerfile: ./docker/Dockerfile.minio
    command: ["minio", "server", "/data", "--console-address", ":9001"]
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - ./data:/data/